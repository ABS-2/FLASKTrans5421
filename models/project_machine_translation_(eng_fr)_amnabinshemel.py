# -*- coding: utf-8 -*-
"""Project_Machine_Translation_(ENG_FR)_AmnaBinshemel

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yb1WK2EaRJ1ActL8ec5u-W3rl454m0Gu

##Machine Translation Using a Seq2Seq Architecture
Â© 2024, Zaka AI, Inc. All Rights Reserved.

---
The goal of this colab is to get you more familiar with the Seq2Seq models and their challenges. For this reason, you will be working on machine translation problem where we would have a sentence as input (in english), and the output is gonna be the translated sentence (in french). So just like what happens with Google Translate.

**Just to give you a heads up:** We won't be having a model performing like Google translate, but at least we will have an idea about how Google Translate works and the challenges that exist with a translation problem.

## Importing Libraries

We start by importing numpy and pandas and then we can add the rest
"""

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Dense, RepeatVector, TimeDistributed
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import string
import matplotlib.pyplot as plt

"""We clone the github repository where our data exists. Here is the github link: https://github.com/zaka-ai/machine_learning_certification/tree/main/Challenge%207

## Getting the data

We read the english sentences in a dataframe named "english", and the french sentences in a dataframe named "french"
"""

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/zaka-ai/machine_learning_certification
# %cd /content/machine_learning_certification/Challenge 7

english = pd.read_csv('en.csv', header=None, names=['English'])

french = pd.read_csv('fr.csv', header=None, names=['French'])

"""**How many sentences does each of the files contain?**"""

print(f"Number of English sentences: {len(english)}")
print(f"Number of French sentences: {len(french)}")

"""Now let us concatenate the 2 dataframes into one dataframe that we call **df** where one column has the english senetnces and the other has the french sentences"""

df = pd.concat([english, french], axis=1)

"""Let's name the columns as **English** and **French** so that we access them easier."""

df.columns = ['English', 'French']

"""Pick a sentence and print it in both languages"""

index = 0
print(f"English: {df['English'][index]}")
print(f"French: {df['French'][index]}")

"""##Cleaning Data

The data that we have is almost clean as we can see, we just need to remove the punctuations inside of it.
"""

def clean_text(text):
    text = text.lower()
    text = text.translate(str.maketrans('', '', string.punctuation))
    return text
    df['English'] = df['English'].apply(clean_text)
    df['French'] = df['French'].apply(clean_text)

"""Make sure that the punctuation is removed by printing the example that you printed earlier."""

example_index = 0
print(f"Cleaned English: {df['English'][example_index]}")
print(f"Cleaned French: {df['French'][example_index]}")

"""##Exploring the Data

Add a column **ENG Length** to the dataset that shows how many words does a sentence contain, and do the same for french in a column called **FR Length**
"""

df['ENG Length'] = df['English'].apply(lambda x: len(x.split()))

df['FR Length'] = df['French'].apply(lambda x: len(x.split()))

"""Visualize the distribution of the lengths of english sentences and french sentences."""

plt.hist(df['ENG Length'], bins=20, alpha=0.5, label='English')
plt.legend(loc='upper right')
plt.xlabel('Sentence Length')
plt.ylabel('Frequency')
plt.title('English Sentence Length Distribution')
plt.show()

plt.hist(df['FR Length'], bins=20, alpha=0.5, label='French', color='orange')
plt.legend(loc='upper right')
plt.xlabel('Sentence Length')
plt.ylabel('Frequency')
plt.title('French Sentence Length Distribution')
plt.show()

"""Get the maximum length of an english sentence and the maximum length of a french sentence."""

max_length_eng = df['ENG Length'].max()
print(f"Maximum English sentence length: {max_length_eng}")

max_length_fr = df['FR Length'].max()
print(f"Maximum French sentence length: {max_length_fr}")

"""##Preprocessing the Data

In order for the data to be fed to the model, it has to be tokenized and padded.

####Tokenization

**To tokenize english and french sentences, we can use only one tokenizer. True or False?**

[Share Your Zaka]

Tokenize the sentences that we have.
"""

def tokenize_sentences(sentences):
    tokenizer = Tokenizer()
    tokenizer.fit_on_texts(sentences)
    sequences = tokenizer.texts_to_sequences(sentences)
    return tokenizer, sequences

english_tokenizer, english_sequences = tokenize_sentences(df['English'])
french_tokenizer, french_sequences = tokenize_sentences(df['French'])

"""**How many unique words do we have in english and in french?**"""

vocab_size_eng = len(english_tokenizer.word_index) + 1
vocab_size_fr = len(french_tokenizer.word_index) + 1

print(f"Number of unique English words: {vocab_size_eng}")
print(f"Number of unique French words: {vocab_size_fr}")

"""####Padding

**What should be the length of the sequences that we have after padding?**
"""

max_length_eng = max(len(seq) for seq in english_sequences)
max_length_fr = max(len(seq) for seq in french_sequences)

print(f"Maximum sequence length for English: {max_length_eng}")
print(f"Maximum sequence length for French: {max_length_fr}")

"""Perform padding on the sequences that we have."""

padded_eng = pad_sequences(english_sequences, maxlen=max_length_eng, padding='post')
padded_fr = pad_sequences(french_sequences, maxlen=max_length_fr, padding='post')

print("English sequences after padding:\n", padded_eng[:2])
print("French sequences after padding:\n", padded_fr[:2])

"""##Modeling

After preprrocessing the data, we can build our model. Start by building a baseline architecture relying on one directional RNNs, LSTMs, or GRUs. It will be good to lookup how to build Seq2Seq models, there are some new layers that will help you like RepeatVector and TimeDistributed.
"""

input_seq = Input(shape=(max_length_eng,))
embedded_seq = Dense(128, activation='relu')(input_seq)
reshaped_seq = tf.keras.layers.Reshape((-1, 128))(embedded_seq)
encoded_seq = LSTM(256)(reshaped_seq)
repeated_seq = RepeatVector(max_length_fr)(encoded_seq)
decoded_seq = LSTM(256, return_sequences=True)(repeated_seq)
output_seq = TimeDistributed(Dense(vocab_size_fr, activation='softmax'))(decoded_seq)
model = Model(input_seq, output_seq)
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.summary()

"""Compile and train the model.
**FYI:** While specifying the architecture of your model and the number of epochs for training, keeep in your mind that your model might take A LOT of time to train.
"""

# Prepare the target output for training
french_output = np.expand_dims(padded_fr, -1)

# Train the model
history = model.fit(
    padded_eng, french_output,
    batch_size=64,
    epochs=10,
    validation_split=0.2
)

"""Define a function that gets an input sentence in english and gives the output sentence in the french language."""

def translate_sentence(model, tokenizer_eng, tokenizer_fr, sentence):
    sequence = tokenizer_eng.texts_to_sequences([sentence])
    padded_seq = pad_sequences(sequence, maxlen=max_length_eng, padding='post')
    prediction = model.predict(padded_seq)
    translated_sentence = ' '.join(
        tokenizer_fr.index_word[idx] for idx in prediction.argmax(axis=2)[0] if idx != 0
    )
    return translated_sentence

"""Test the following sentence"""

# Test translation with baseline model
input_sentence = "she is driving the truck"
print("Input Sentence:", input_sentence)
print("Translated Sentence:", translate_sentence(model, english_tokenizer, french_tokenizer, input_sentence))

"""Try to improve your model by modifying the architecture to take into account bidirectionality which is very useful in Machine Translation. Create a new model called model2"""

from tensorflow.keras.layers import Embedding, Bidirectional
french_output = np.expand_dims(padded_fr, -1)
# Step 10: Build the improved Seq2Seq model
input_seq = Input(shape=(max_length_eng,))

# Embedding Layer
embedding_layer = Embedding(input_dim=vocab_size_eng, output_dim=256, input_length=max_length_eng)(input_seq)

# Encoder with Stacked Bidirectional LSTM
encoded_seq = Bidirectional(LSTM(256, return_sequences=True))(embedding_layer)
encoded_seq = LSTM(256)(encoded_seq)

# Decoder with RepeatVector and Stacked LSTM
repeated_seq = RepeatVector(max_length_fr)(encoded_seq)
decoded_seq = LSTM(256, return_sequences=True)(repeated_seq)
decoded_seq = LSTM(256, return_sequences=True)(decoded_seq)

# Output Layer
output_seq = TimeDistributed(Dense(vocab_size_fr, activation='softmax'))(decoded_seq)

model = Model(input_seq, output_seq)
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.summary()

"""compile and train your new model."""

from tensorflow.keras.callbacks import EarlyStopping # Import EarlyStopping

early_stopping = EarlyStopping(monitor='val_loss', patience=5)
history = model.fit(
    padded_eng, french_output,
    batch_size=64,
    epochs=50,
    validation_split=0.2,
    callbacks=[early_stopping]
)

"""Define a new function that relies on your new model to make predictions."""

def translate_sentence(model, tokenizer_eng, tokenizer_fr, sentence):
    sequence = tokenizer_eng.texts_to_sequences([sentence])
    padded_seq = pad_sequences(sequence, maxlen=max_length_eng, padding='post')
    prediction = model.predict(padded_seq)
    translated_sentence = ' '.join(
        tokenizer_fr.index_word[idx] for idx in prediction.argmax(axis=2)[0] if idx != 0
    )
    return translated_sentence

input_sentence = "she is driving the truck"
print("Input Sentence:", input_sentence)
translated_sentence = translate_sentence(model, english_tokenizer, french_tokenizer, input_sentence)
print("Translated Sentence:", translated_sentence)

"""**What is another adjustment in terms of architecture that you might be able to do to improve your model?**

*Use an Embedding layer to learn word representations or increase the number of LSTM layers for better feature extraction.*

**What are some additional ways that we can do to improve the performance of our model?**

*Implement attention mechanisms, train on a larger dataset, or fine-tune hyperparameters such as learning rate and batch size.*
"""

import io
import pickle
from google.colab import files
import uuid  # Import the uuid module for generating unique filenames

# Save the model to a BytesIO object
model_bytes = io.BytesIO()
pickle.dump(model, model_bytes)
model_bytes.seek(0)  # Reset the pointer to the beginning

# Generate a unique filename using uuid
unique_filename = f"model_{uuid.uuid4().hex}.pkl"

# Save the model to a file in Colab's environment
with open(unique_filename, 'wb') as f:
    f.write(model_bytes.read())

# Download the file using files.download()
files.download(unique_filename)